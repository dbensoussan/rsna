{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from albumentations import Compose, ShiftScaleRotate, Resize, Normalize, CenterCrop, HorizontalFlip\n",
    "from albumentations import Rotate, RandomBrightness, RandomContrast\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "import xception\n",
    "import util\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "dir_train_img = 'data/images/stage_1_train_images_jpg'\n",
    "dir_test_img = 'data/images/stage_1_test_images_jpg'\n",
    "dir_model = 'models'\n",
    "dir_labels = 'data/labels'\n",
    "dir_submission = 'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created train (len: 476840) and val (len: 158947) sets containing around 15% positives.\n"
     ]
    }
   ],
   "source": [
    "util.train_val_files(pd.read_csv('data/labels/train_filtered.csv'),\n",
    "                positive_rate=None, test_size=.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data loader\n",
    "class IntracranialDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, path, labels, transform=None):\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.path, self.data.loc[idx, 'ID'] + '.jpg')\n",
    "        img = mpimg.imread(img_name)\n",
    "        \n",
    "        if self.transform:       \n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']   \n",
    "            \n",
    "        img = img[None, :, :] # Adds a dimension to the array\n",
    "        img = np.repeat(img, 3, 0) # Copies the grayscale image to the 3 RGB channels\n",
    "        \n",
    "        if self.labels:\n",
    "            labels = torch.tensor(\n",
    "                self.data.loc[idx, ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural', 'any']])\n",
    "            return {'image': img, 'labels': labels}    \n",
    "        else:      \n",
    "            return {'image': img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 6\n",
    "n_labels = 6\n",
    "batch_size = 16\n",
    "\n",
    "#resize = 333 # Xception\n",
    "resize = 320 #resnext\n",
    "\n",
    "data_transforms = {\n",
    "    'train': Compose([Resize(resize, resize),\n",
    "                      CenterCrop(299, 299),\n",
    "                      #HorizontalFlip(p=.2),\n",
    "                      ShiftScaleRotate(p=.5, rotate_limit=20),\n",
    "                      #Rotate(p=0.5, limit=20),\n",
    "                      #RandomBrightness(p=0.5, limit=0.2),\n",
    "                      #RandomContrast(p=0.5, limit=0.2),\n",
    "                      ToTensor(),\n",
    "                      #Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                     ]),\n",
    "    'val': Compose([Resize(resize, resize),\n",
    "                    CenterCrop(299, 299),\n",
    "                    ToTensor(),\n",
    "                    #Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                   ]),\n",
    "    'test': Compose([Resize(resize, resize),\n",
    "                    CenterCrop(299, 299),\n",
    "                    ToTensor(),\n",
    "                    #Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                    ])\n",
    "}\n",
    "\n",
    "train_dataset = IntracranialDataset(\n",
    "    csv_file=os.path.join(dir_labels, 'train.csv'),\n",
    "    path=dir_train_img,\n",
    "    transform=data_transforms['train'],\n",
    "    labels=True)\n",
    "\n",
    "val_dataset = IntracranialDataset(\n",
    "    csv_file=os.path.join(dir_labels, 'val.csv'),\n",
    "    path=dir_train_img,\n",
    "    transform=data_transforms['val'],\n",
    "    labels=True)\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "              'val': torch.utils.data.DataLoader(\n",
    "                  val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)}\n",
    "\n",
    "dataset_sizes = {'train': len(train_dataset),\n",
    "                'val': len(val_dataset)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10, image_max=1000):\n",
    "    \"\"\"\n",
    "    Image max is an optional parameter that can be used to prematurely finish the epoch training phase\n",
    "    and start the validation phase when the number of images processed is reached.\n",
    "    \"\"\"\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1.0\n",
    "    \n",
    "    # We give more weight to the error on the \"any\" label like in the evaluation method for this Kaggle\n",
    "    label_weights = torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.2]).to(device)\n",
    "    sum_weights = torch.sum(label_weights)\n",
    "    \n",
    "    # Below x% error in predicted probability we consider the output correct for statistical purposes\n",
    "    dist_from_target_treshold = .05\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:          \n",
    "            print('{} phase :'.format(phase))\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_loss_batch = 0.0\n",
    "            time_start_batch = time.time()\n",
    "            time_elapsed_batch = 0.0\n",
    "            img_processed_batch, img_processed_total = 0, 0\n",
    "            refresh_every_n_batches = 5\n",
    "            newline_every_n_refreshes = 100\n",
    "            \n",
    "            # Iterate over data\n",
    "            for i, sample_batch in enumerate(dataloaders[phase]):\n",
    "                if i % (refresh_every_n_batches * newline_every_n_refreshes) == (\n",
    "                    refresh_every_n_batches * newline_every_n_refreshes - 1):\n",
    "                    print()\n",
    "                    running_loss_batch = 0.0\n",
    "                    img_processed_batch = 0\n",
    "                    time_start_batch = time.time()\n",
    "                \n",
    "                inputs = sample_batch['image'].to(device)\n",
    "                labels = sample_batch['labels'].float().to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = (loss * label_weights * n_labels / sum_weights).mean()\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_loss_batch += loss.item() * inputs.size(0)\n",
    "                \n",
    "                pred_error = torch.abs(torch.sigmoid(outputs) - labels)\n",
    "                running_corrects += torch.sum(pred_error < dist_from_target_treshold) / n_labels\n",
    "                \n",
    "                img_processed_total += batch_size\n",
    "                img_processed_batch += batch_size\n",
    "\n",
    "                if i % refresh_every_n_batches == (refresh_every_n_batches - 1):\n",
    "                    now = time.time()\n",
    "                    time_elapsed_batch = now - time_start_batch\n",
    "            \n",
    "                    print('Image {}/{} ({:.1f}%). Loss: {:.4f}  Time: {:.0f}m {:.0f}s ({:.1f} images/sec)'.format(\n",
    "                        img_processed_total, dataset_sizes[phase],\n",
    "                        100*img_processed_total / dataset_sizes[phase],\n",
    "                        running_loss_batch / img_processed_batch,\n",
    "                        time_elapsed_batch // 60, time_elapsed_batch % 60,\n",
    "                        img_processed_batch / time_elapsed_batch), end='\\r')\n",
    "                else:\n",
    "                    print('Image {}/{}'.format(img_processed_total, dataset_sizes[phase]), end='\\r')           \n",
    "                    \n",
    "                if image_max is not None:\n",
    "                    if img_processed_total >= image_max:\n",
    "                        break\n",
    "                    if phase == 'val':\n",
    "                        if img_processed_total >= image_max * .4:\n",
    "                            break\n",
    "\n",
    "            epoch_loss = running_loss / img_processed_total\n",
    "            epoch_acc = running_corrects.double() / img_processed_total\n",
    "\n",
    "            print()\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val':\n",
    "                # Note that step should be called after validate()\n",
    "                scheduler.step(epoch_loss)\n",
    "                \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Time elapsed {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        print()\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a model\n",
    "model_name = 'resnext50_32x4d'\n",
    "\n",
    "model = models.resnext50_32x4d(pretrained=True, progress=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# model_name = 'xception'\n",
    "\n",
    "# model = xception.xception()\n",
    "# num_ftrs = model.last_linear.in_features\n",
    "# model.last_linear = nn.Linear(num_ftrs, n_classes)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "plist = [{'params': model.parameters(), 'lr': 2e-5}]\n",
    "optimizer = optim.Adam(plist)\n",
    "exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if needed\n",
    "score = '0.0855' # Manually select the model to load\n",
    "model_path = os.path.join(dir_model, '{}_{}.tar'.format(model_name, score)) \n",
    "util.load_model(model_path, model, optim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "train phase :\n",
      "Image 7984/476840 (1.7%). Loss: 0.0801  Time: 8m 26s (15.7 images/sec)\n",
      "Image 15984/476840 (3.3%). Loss: 0.0788  Time: 8m 23s (15.8 images/sec)\n",
      "Image 23984/476840 (5.0%). Loss: 0.0781  Time: 8m 22s (15.8 images/sec)\n",
      "Image 31984/476840 (6.7%). Loss: 0.0758  Time: 8m 21s (15.8 images/sec)\n",
      "Image 39984/476840 (8.4%). Loss: 0.0775  Time: 8m 20s (15.9 images/sec)\n",
      "Image 47984/476840 (10.0%). Loss: 0.0738  Time: 8m 20s (15.9 images/sec)\n",
      "Image 55984/476840 (11.7%). Loss: 0.0778  Time: 8m 20s (15.9 images/sec)\n",
      "Image 60000/476840 (12.6%). Loss: 0.0788  Time: 4m 13s (15.9 images/sec)\n",
      "train Loss: 0.0776 Acc: 0.8615\n",
      "val phase :\n",
      "Image 7984/158947 (5.0%). Loss: 0.0773  Time: 2m 40s (49.6 images/sec)\n",
      "Image 15984/158947 (10.0%). Loss: 0.0736  Time: 2m 40s (49.7 images/sec)\n",
      "Image 23984/158947 (15.0%). Loss: 0.0792  Time: 2m 39s (49.8 images/sec)\n",
      "Image 24000/158947 (15.1%). Loss: 0.1430  Time: 0m 0s (82.7 images/sec)\n",
      "val Loss: 0.0766 Acc: 0.8653\n",
      "Time elapsed 71m 17s\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "train phase :\n",
      "Image 7984/476840 (1.7%). Loss: 0.0734  Time: 8m 17s (15.9 images/sec)\n",
      "Image 15984/476840 (3.3%). Loss: 0.0726  Time: 8m 19s (15.9 images/sec)\n",
      "Image 23984/476840 (5.0%). Loss: 0.0743  Time: 8m 19s (15.9 images/sec)\n",
      "Image 31984/476840 (6.7%). Loss: 0.0769  Time: 8m 19s (15.9 images/sec)\n",
      "Image 39984/476840 (8.4%). Loss: 0.0725  Time: 8m 20s (15.9 images/sec)\n",
      "Image 47984/476840 (10.0%). Loss: 0.0703  Time: 8m 20s (15.9 images/sec)\n",
      "Image 55984/476840 (11.7%). Loss: 0.0759  Time: 8m 21s (15.9 images/sec)\n",
      "Image 60000/476840 (12.6%). Loss: 0.0626  Time: 4m 13s (15.9 images/sec)\n",
      "train Loss: 0.0729 Acc: 0.8665\n",
      "val phase :\n",
      "Image 7984/158947 (5.0%). Loss: 0.0721  Time: 2m 39s (49.8 images/sec)\n",
      "Image 15984/158947 (10.0%). Loss: 0.0686  Time: 2m 39s (49.8 images/sec)\n",
      "Image 23984/158947 (15.0%). Loss: 0.0719  Time: 2m 39s (49.9 images/sec)\n",
      "Image 24000/158947 (15.1%). Loss: 0.0506  Time: 0m 0s (79.8 images/sec)\n",
      "val Loss: 0.0708 Acc: 0.8790\n",
      "Time elapsed 142m 16s\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "train phase :\n",
      "Image 7984/476840 (1.7%). Loss: 0.0745  Time: 8m 17s (15.9 images/sec)\n",
      "Image 15984/476840 (3.3%). Loss: 0.0684  Time: 8m 19s (15.9 images/sec)\n",
      "Image 23984/476840 (5.0%). Loss: 0.0733  Time: 8m 19s (15.9 images/sec)\n",
      "Image 31984/476840 (6.7%). Loss: 0.0691  Time: 8m 20s (15.9 images/sec)\n",
      "Image 39984/476840 (8.4%). Loss: 0.0698  Time: 8m 20s (15.9 images/sec)\n",
      "Image 47984/476840 (10.0%). Loss: 0.0706  Time: 8m 20s (15.9 images/sec)\n",
      "Image 55984/476840 (11.7%). Loss: 0.0688  Time: 8m 20s (15.9 images/sec)\n",
      "Image 60000/476840 (12.6%). Loss: 0.0780  Time: 4m 13s (15.9 images/sec)\n",
      "train Loss: 0.0711 Acc: 0.8693\n",
      "val phase :\n",
      "Image 7984/158947 (5.0%). Loss: 0.0688  Time: 2m 41s (49.1 images/sec)\n",
      "Image 15984/158947 (10.0%). Loss: 0.0641  Time: 2m 39s (50.0 images/sec)\n",
      "Image 23984/158947 (15.0%). Loss: 0.0653  Time: 2m 39s (49.9 images/sec)\n",
      "Image 24000/158947 (15.1%). Loss: 0.1292  Time: 0m 0s (79.8 images/sec)\n",
      "val Loss: 0.0659 Acc: 0.8763\n",
      "Time elapsed 213m 16s\n",
      "\n",
      "Best val loss: 0.065873\n"
     ]
    }
   ],
   "source": [
    "# Updates all parameters in the CNN\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model, loss = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=3, image_max=60000)\n",
    "\n",
    "# Save model with loss information\n",
    "util.save_model(model, optimizer, loss, model_name, dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, tta_size=1):\n",
    "    \"\"\"\n",
    "    tta_size is a parameter used to apply test time augmentation on images.\n",
    "    The value of the parameter (when > 1) is the number of augmentations applied on\n",
    "    each image.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tta_size == 1:\n",
    "        test_transform = data_transforms['test']\n",
    "    elif tta_size > 1:\n",
    "        test_transform = data_transforms['train']\n",
    "    else:\n",
    "        raise ValueError('Number of folds for test-time augmentation must be >= 1.')\n",
    "        \n",
    "    test_dataset = IntracranialDataset(\n",
    "        csv_file='test.csv', path=dir_test_img, transform=test_transform, labels=False)\n",
    "    dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    dataset_sizes['test'] = len(test_dataset)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = np.zeros((dataset_sizes['test'] * n_classes, tta_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(tta_size):\n",
    "            for i, sample_batch in enumerate(dataloaders['test']):\n",
    "                inputs = sample_batch['image'].to(device)\n",
    "                pred = model(inputs)\n",
    "\n",
    "                y_pred[(i * batch_size * n_classes):((i + 1) * batch_size * n_classes), k:k+1] = torch.sigmoid(\n",
    "                    pred).detach().cpu().reshape((len(inputs) * n_classes, 1))\n",
    "\n",
    "                print('TTA {}: Image {}/{}'.format(k+1, i * batch_size, dataset_sizes['test']), end='\\r')\n",
    "\n",
    "        # Averaging the predictions for the different augmentations of the image tested\n",
    "        y_pred = y_pred.mean(axis=1)\n",
    "    print()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA 5: Image 78544/78545\n"
     ]
    }
   ],
   "source": [
    "test_pred = model_inference(model, tta_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the predictions in a file\n",
    "file_name = '{}_{:.5f}.csv'.format(model_name, loss)\n",
    "util.create_prediction_csv(test_pred, file_name, dir_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
